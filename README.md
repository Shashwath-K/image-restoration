# Image Restoration System

## Project
**A Unified Deep Learning Framework for Multi-Task Image and Video Restoration**  
(Inpainting, Frame Interpolation, Colorization, and Denoising)

---

## Overview
This repository implements a modular pipeline for restoring degraded imagery and video. The system integrates four primary restoration tasks:

1. **Image Inpainting** â€” fill missing regions and holes.  
2. **Frame Interpolation** â€” reconstruct missing video frames and smooth temporal discontinuities.  
3. **Colorization** â€” convert grayscale images/frames to plausible color.  
4. **Denoising** â€” remove sensor noise, motion blur, and compression artifacts.

The architecture is intentionally minimal to accelerate experimentation and demoing while remaining extensible for research and production workflows.

---

## Features
- Modular service wrappers per task (PyTorch-based).  
- Simple Gradio demo (`app.py`) for uploads, task selection and visualization.  
- Metrics for objective evaluation: PSNR, SSIM, LPIPS (optional).  
- Example pipelines (single-task or chained multi-task restoration).  
- Sample datasets and utilities for IO and visualization.

---

## Quickstart (local / Colab)
1. Clone the repository:
   ```bash
   git clone <your-repo-url>
   cd image-restoration-system
   ```

2. Install Dependencies:
   ``` bash
   python -m venv venv
    source venv/bin/activate         # Linux / macOS
    venv\Scripts\activate            # Windows (PowerShell)
    pip install -r requirements.txt
   ```
3. Run the Gradio demo:
   ``` py
     python app.py
   ```
4. Open the provided Gradio URL in your browser.
   
## File Structure

``` text
image-restoration-system/
â”‚
â”œâ”€â”€ app.py                     # Gradio UI entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ inpainting/            # LaMa weights / configs
â”‚   â”œâ”€â”€ interpolation/         # RIFE weights / configs
â”‚   â”œâ”€â”€ colorization/          # ECCV16 weights / code
â”‚   â””â”€â”€ denoising/             # DnCNN / pytorch weights
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ router.py              # Task decision & chaining logic
â”‚   â””â”€â”€ processor.py           # Inference orchestration
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ inpaint_service.py
â”‚   â”œâ”€â”€ interpolate_service.py
â”‚   â”œâ”€â”€ colorize_service.py
â”‚   â””â”€â”€ denoise_service.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ io_utils.py
â”‚   â””â”€â”€ metrics.py
â”‚
â””â”€â”€ data/
    â””â”€â”€ samples/
```

### Models
LaMa â€” resolution-robust large-mask image inpainting.
Repository: https://github.com/advimman/lama

RIFE â€” real-time intermediate flow estimation for frame interpolation.
Repository: https://github.com/hzwer/ECCV2022-RIFE

ECCV16 â€” Colorful Image Colorization (Zhang et al.).
Repository: https://github.com/richzhang/colorization

DnCNN â€” residual learning CNN for image denoising.
Repository: https://github.com/cszn/DnCNN

### Datasets

Inpainting: Places2, CelebA-HQ (use curated subsets for experiments).

Interpolation: Vimeo90K triplet dataset or custom frame pairs.

Colorization: ImageNet / Places365 (use grayscale conversions for supervised training).

Denoising: BSD400, SIDD (real-noise).


1. Peak Signal-to-Noise Ratio (PSNR)

For two images ğ‘‹ (reference) and ğ‘‹^ (reconstruction) with ğ‘€Ã—ğ‘ pixels and maximum pixel value ğ‘€ğ´ğ‘‹ (e.g., 255):<br/>
![Peak Signal to Noise Ratio]!(assets/image.png)

2. Structural SIMilarity Index (SSIM)

SSIM compares luminance, contrast and structure between local windows. For patches ğ‘¥,ğ‘¦:<br/>
![Structural Similarity Index](assets/image-1.png)

Mean SSIM (over the image) is the reported metric. SSIM âˆˆ [âˆ’1, 1], but practically in [0,1] for natural images.

3. Learned Perceptual Image Patch Similarity (LPIPS)

LPIPS uses deep network features ğœ™(â‹…) to measure perceptual distance:<br/>
![Learned Perceptual Image Patch Similarity](assets/image-2.png)

where ğ‘™ indexes layers, ğ‘¤ğ‘™ are learned layer-wise weights and ğœ™^ denotes normalized activations. Lower LPIPS is better (closer perceptual similarity).

